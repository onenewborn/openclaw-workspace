# 2026-02-06 工作日志

## 00:00 Heartbeat 检查
- 新的一天开始
- 创建今日 memory 文件
- 检查任务状态：5个Ready任务，无阻塞

## 13:00 哈尔滨旅游准备讨论

### 用户状态
- 刚恢复肠胃炎，上午在休息
- 明天（2月7日）下午5点飞哈尔滨
- 有很多DDL，但提不起精神

### 旅游计划分析
用户发送了详细的7日游PDF计划（2月7日-2月13日），包含：
- D1-D3：哈尔滨市区（中央大街、冰雪大世界、极地馆等）
- D4-D6：横道河子（滑雪、东北虎林园）
- D7：返程

### 景点预约查询结果
我帮用户查询了各个景点的预约要求：

**必须提前预约：**
- 冰雪大世界（建议提前购票，可能限流）
- 东北虎林园（建议提前1天，惊险车可能售罄）
- 索菲亚教堂内部（当天12点后不能订当天）

**可以现场购买：**
- 极地公园（淘学企鹅巡游免费，每天13:30）
- 太阳岛雪博会
- 横道河子滑雪场

### 关键提醒
- 淘学企鹅巡游免费，在极地广场13:30
- 冰雪大世界节假日可能限流，建议尽快买票
- 横道河子高铁票需要确认是否已买

## 15:00 选课纠结讨论

### 用户情况
- 研二第二学期，即将毕业
- 选了三门课，需要退掉一门
- 纠结于：Python编程 vs 统计推断

### 两门课对比分析

| 维度 | Python编程 | 统计推断 |
|------|-----------|---------|
| 内容 | numpy/pandas/实际编程 | 统计公式/不等式/理论证明 |
| 复习难度 | 中等（偏实践） | 高（偏理论，需推导记忆） |
| 对工作实用度 | 极高（每天都在用） | 中等（理论背景） |
| 自学可行性 | 高 | 低 |

### 我的建议
**退统计推断，留Python编程**

理由：
1. Python是AI工程师的饭碗，工作中每天都在用
2. 统计推断可以"先退后补"，工作中用到可以查书/论文
3. 研二下时间宝贵，统计推断需要大精力复习，性价比不高
4. Python偏实践，考试容错率更高

### 退课前需确认
1. 学分是否还满足毕业要求
2. 哪门课老师给分更友好
3. 用户统计基础如何
4. 未来工作方向（偏工程落地 vs 算法研究）

## 18:00 飞机阅读清单推荐

### 背景
用户明天（2月7日）有4.5小时飞机，想找论文/代码库/文章看。

### 推荐的阅读清单

**论文类（上下文学习方向）：**
- LongBench v2 - 长上下文多任务benchmark
- 100-LongBench - 长context评测新指标
- LoCoBench - 长上下文+软件工程

**代码库类（知识图谱+LLM）：**
- Microsoft GraphRAG - 图谱检索增强生成
- LangChain Graph RAG - 知识图谱+LLM实现
- LlamaIndex - 检索增强标准工具

**电子书/博客类：**
- 《The Book of Why》前3章 - 因果推理思维框架
- Sebastian Raschka 2025 LLM论文清单
- 美团技术团队博客（知识图谱/金融风控）

### 三个方案
- 方案A：上下文学习深度版（论文+代码+博客）
- 方案B：工程实践版（源码+书籍+笔记）
- 方案C：轻松版（文档+博客+休息）

### 出发前准备
```bash
git clone https://github.com/microsoft/graphrag.git
git clone https://github.com/langchain-ai/langchain.git
# 下载论文PDF到本地
```

## 22:00 Sebastian Raschka 介绍

### 用户问题
"Sebastian Raschka 是谁，我们要看他的论文清单吗？"

### 调查结果
**Sebastian Raschka** 是 ML/LLM 领域非常有名的研究者和教育者：
- 现任 AI Research Engineer（前威斯康星大学统计学助理教授）
- 10+年AI经验，Google Scholar被引12,000+次
- 代表作：《Build a Large Language Model From Scratch》
- 特点：擅长把复杂技术讲清楚

### 他的2025论文清单
- 2025年上半年(1-6月): 200+篇论文分类整理
- 2025年下半年(7-12月): 推理模型、推理时扩展、架构
- 2025 LLM全景回顾: 从DeepSeek R1到2026预测

**推荐先看**：《The State Of LLMs 2025》综述性质，4.5小时飞机刚好读完

---

## 22:30 2026年最新论文检索（第一轮）

### 用户需求
找2026年最新杰出论文，方向：
1. 推理相关
2. 大模型蒸馏到小模型
3. 长上下文相关

### 执行方式
启动 research sub-agent 使用 Gemini CLI 深度检索

### 结果反馈
❌ **用户不满意**：我找了很多2023-2025年的老论文（如DeepSeek-R1是2025年1月，YaRN是2023年），而不是真正的2026年新论文

**用户批评**: "我真的服了，deepseek-r1都是多老的论文了"

### 问题反思
- 混淆了"经典必读"和"2026最新"
- 2026年2月初，真正的新论文还不多（论文有发表周期）
- 应该找2025年12月-2026年1月的"年终/年初爆发"论文

---

## 22:40 2026年最新论文检索（第二轮 - 修正版）

### 修正后的范围
用户明确指定：
1. **2025年12月-2026年1月**的"年终/年初爆发"论文
2. **ICLR 2026**投稿（刚截稿，preprint刚放出）

### 找到的真正2026年论文
| 论文 | arXiv | 核心亮点 |
|------|-------|---------|
| SokoBench | 2601.20856 | 评估长时程规划和推理能力，用DeepSeek R1和GPT-5测试 |
| Test-Time Intervention | 2601.11252 | 不扩模型规模，用推理时干预实现高效深度推理 |

### 正在执行
- 启动第二轮 research sub-agent
- 严格限定时间范围：2025年12月-2026年1月
- 聚焦ICLR 2026投稿
- 排除所有2025年1月及更早的论文

---

## 📊 今日关键决策总结

### 做出的决策
1. **选课建议**: 退统计推断，留Python编程（实用性>理论深度）
2. **论文检索范围**: 从"2026最新"修正为"2025年12月-2026年1月+ICLR 2026"

### 待办事项更新
- [ ] 等待第二轮论文检索结果
- [ ] 为用户整理真正的2026年最新论文清单（飞机上看）
- [ ] 明天2月7日用户飞哈尔滨，注意旅游提醒

### 教训记录
⚠️ **避免再次犯错**: 用户要"2026最新"时，不要给2023-2025年的经典论文。2026年2月初真正新论文很少，应提前说明时间范围问题。

## 23:00 三篇2026年1月新论文深度分析

### 用户发送的三篇论文
用户连续发了三篇**真正2026年1月**的arXiv论文（都是刚挂出不久的）：

| 论文 | 机构 | 核心问题 | 解决方案 | 与用户工作相关性 |
|------|------|---------|---------|---------------|
| **AWorld** | Inclusion AI / 西湖大学 / 上海创新研究院 | Agent训练经验生成太慢 | 分布式架构加速14.6倍，Qwen3-32B超越GPT-4o (GAIA 32.23%) | ⭐⭐⭐⭐⭐ Agent-RL基础设施 |
| **EverMemOS** | 盛达集团 | 长期交互记忆碎片化，无法整合经验 | 记忆操作系统：MemCells → MemScenes → 重构式回忆 | ⭐⭐⭐⭐⭐ 长期记忆管理 |
| **ArenaRL** | 阿里巴巴通义实验室 | 开放式任务奖励信号噪声大（判别性崩溃） | 锦标赛制相对排名替代逐点评分，O(n²)→O(n) | ⭐⭐⭐⭐⭐ 开放式任务RL训练 |

### 三篇论文的核心价值

**1. AWorld - "learning from practice"**
- 完整的Agent训练流水线：环境交互 → 经验生成 → RL训练
- 分布式架构解决经验生成瓶颈（14.6倍加速）
- 与用户Leader提到的"Web Deep Research"和"Agent-RL"方向直接相关

**2. EverMemOS - 长期记忆管理**
- 解决"Lost in the Middle"和记忆碎片化问题
- 三层架构：Episodic Trace Formation → Semantic Consolidation → Reconstructive Recollection
- 受生物记忆痕迹(engram)启发
- 金融知识图谱需要长期追踪用户画像，与此强相关

**3. ArenaRL - 开放式任务RL**
- 解决奖励模型"判别性崩溃"（discriminative collapse）
- 逐点评分→组内相对排名（锦标赛制）
- 种子单败淘汰机制，用贪婪解码轨迹作为"质量锚点"
- Web Deep Research没有标准答案，正好需要这种开放式RL

### 飞机阅读建议
**4.5小时最优组合**：
1. AWorld (2小时) - 完整的Agent训练流水线
2. EverMemOS (1.5小时) - 长期记忆管理
3. ArenaRL (1小时) - 快速浏览锦标赛机制核心思想

### 关键洞察
- 三篇都是**中文/华人团队**（西湖大学、盛达、阿里）
- 都**开源**了代码
- 都与用户即将入职的**美团金融知识图谱**工作高度相关
- 代表了2026年Agentic AI的最新方向

---

*记录于 2026-02-06 23:30*
